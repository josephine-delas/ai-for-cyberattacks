{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 1 -  AI for OSINT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De nombreuse informations sur un profil sont aujourd'hui disponibles sur internet grâce aux réseaux sociaux. Ces informations peuvent permettre à un potentiel attaquant d'identifier les cibles les plus pertinentes pour lancer une attaque de spear phishing ou faire du social engineering.\n",
    "\n",
    "Cependant, l'investigation en ligne (OSINT) est coûteuse en temps humain : les cibles sont de plus en plus nombreuses et la quantité d'information disponible explose. Les méthodes d'IA permettent aux attaquants d'automatiser cette première phase de reconnaissance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Clustering Twitter Users*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour trouver les cibles les plus susceptible de mordre à l'hameçon, la première étape de la pipeline consiste en un algorithme de clustering qui place un utilisateur donné dans la catégorie \"High Value Target\" ou non. \n",
    "\n",
    "Dans ce TP, nous allons nous intéresser aux jeux olympiques de Tokyo 2020."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce TP est divisé en 5 tâches,  : \n",
    "- Récupérer et comprendre les données utilisées\n",
    "- Définir les caractéristiques à extraire (feature engineering)\n",
    "- Sélection et paramétrisation d'un modèle\n",
    "- Évaluation des performances\n",
    "- Itération des modèles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mise en place de l'environnement"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de commencer, commencez par créer et activer un environnement virtuel Python qui servira pour ce TP :\n",
    "```\n",
    "$ python3 -m venv .venv\n",
    "$ source .venv/bin/activate\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installez ensuite les dépendances nécessaire :\n",
    "````\n",
    "$ pip install -r requirements.py\n",
    "````\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définissez l'environnement virtuel comme kernel jupiter, puis exécutez la cellul suivante pour importer les librairies qui nous seront utiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération et compréhension des données"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons travailler avec des données plubliquement accessibles sur Twitter. \n",
    "\n",
    "Nous verrons dans un premier temps comment mettre en place un client pour requêter l'API de twitter et récupérer nous-même les données. \n",
    "\n",
    "Ensuite, nous utiliserons un jeu de données déjà construit, sur les JO de Tokyo. Elles ont été collectées à l'aide de la librairie tweepy pour accéder à l'API de Twitter, en utilisant hashtag pertinent pour le sujet (#Tokyo2020)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mise en place du client"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez créer un compte Twitter, et remplir ci-dessous vos credentials. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twikit.client import Client\n",
    "\n",
    "client = Client(language='en-US')\n",
    "client.login(auth_info_1='username',\n",
    "             password='pwd')\n",
    "client.save_cookies('cookies.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'result'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m client\u001b[39m.\u001b[39mload_cookies(path\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcookies.json\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m user \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mget_user_by_screen_name(\u001b[39m'\u001b[39;49m\u001b[39mjdelas1494059\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(user\u001b[39m.\u001b[39mname)\n",
      "File \u001b[0;32m~/dev/Hexagone/.venv/lib/python3.11/site-packages/twikit/client.py:1120\u001b[0m, in \u001b[0;36mClient.get_user_by_screen_name\u001b[0;34m(self, screen_name)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39muser\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m response[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m   1119\u001b[0m     \u001b[39mraise\u001b[39;00m UserNotFound(\u001b[39m'\u001b[39m\u001b[39mThe user does not exist.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1120\u001b[0m user_data \u001b[39m=\u001b[39m response[\u001b[39m'\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mresult\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m   1121\u001b[0m \u001b[39mif\u001b[39;00m user_data\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39m__typename\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mUserUnavailable\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m   1122\u001b[0m     \u001b[39mraise\u001b[39;00m UserUnavailable(user_data\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[0;31mKeyError\u001b[0m: 'result'"
     ]
    }
   ],
   "source": [
    "client.load_cookies(path='cookies.json')\n",
    "user = client.get_user_by_screen_name()\n",
    "print(user.name)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupération des données"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Définir une requête pertinente pour les données qui nous intéressent, et utiliser les mots-clés `since:YYYY-MM-DD` et `until:YYYY-MM-DD` pour cadrer la date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Écrire une fonction `get_tweets` qui permette de récupérer les tweets correspondant à la requête définie ci-dessus, et de sauvegarder les informations qui nous intéressent dans un fichier csv local. On limitera ici le nombre de tweets téléchargés à 500.\n",
    "    \n",
    "    *Indice : utilisez la fonction `client.search_tweets()`de `twikit` pour requêter l'API. Explorez dans un premier temps les métadonnées qui peuvent être disponibles pour un tweet*\n",
    "\n",
    "    Attention ! L'API a une rate-limit de 500 tweets toutes les 15 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = client.search_tweet(query = query, product='Top', count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=os.path.join('data', 'twikit_tokyo_2020_tweets.csv')\n",
    "\n",
    "columns = []\n",
    "\n",
    "def get_tweets(client, query, filepath=filepath, max_results=500):\n",
    "    tweets = [] \n",
    "    results = client.search_tweet(query = query, product='Top', count=max_results)\n",
    "    tweets+=results\n",
    "    while True:\n",
    "        tweets += results.next()\n",
    "        if len(tweets)>= max_results:\n",
    "            break\n",
    "    with open(filepath, 'w', newline='') as f:\n",
    "        writer = csv.writer(f, delimiter = ',')\n",
    "        writer.writerow(columns)\n",
    "        for tweet in tweets:\n",
    "            writer.writerow(...)\n",
    "\n",
    "    return "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Utiliser `get_tweets()`pour récupérer les tweets qui nous intéressent, et vérifier brièvement que les résultats correspondent à nos attentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.load_cookies(path='cookies.json')\n",
    "get_tweets(client, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[0].text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour aller plus vite, dans la suite du TD, nous utiliserons tous le même jeu de données sur les JO de Tokyo en 2020.\n",
    "Le fichier CSV contenant les colonnes qui nous intéresse est disponible dans le dossier `data`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation des données"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nous avons collecté les données, il est important de les connaître et de les comprendre : sont-elles pertinentes ? Y a-t-il du bruit ? Peut-on le filtrer ? Toutes ces questions nécessitent un inspection minutieuse des résultats obtenus. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. La bibliothèque la plus couramment utilisée pour le traitement de données en Data Science est `pandas`(ref to pandas). Utiliser cet outil pour créer un dataframe contenant les colonnes `'date'`, `'text'` et `'user_name'`.  \n",
    "    \n",
    "    *Indice : la fonction `to_datetime()` est utile pour manipuler plus facilement les données temporelles*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Utiliser les fonctionnalités des dataframes pandas pour visualiser le nombre de tweets par jour de notre ensemble de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Et le nombre de tweets par utilisateur ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec cet échantillon, faisons un peu de traitement de texte brut pour examiner le contenu des tweets. Une première façon simple - et souvent instructive - d'inspecter le contenu des données textuelles consiste à examiner les n-grammes les plus courants. En modélisation linguistique, un \"n-gramme\" est une collection contiguë de quelque n éléments - il s'agit souvent de mots séparés par des espaces. Par exemple, les 3-grammes de la phrase \"Le chien a mangé mes devoirs\" seraient \"le chien a\", \"chien a mangé\", \"a mangé mes\", \"mangé mes devoirs\".\n",
    "\n",
    "Nous allons donc afficher les 3-grammes les plus fréquents, afin de se faire une idée des données textuelles dont nous disposons. Si nous voyons un contenu qui ne nous semble pas pertinent, nous pouvons revenir en arrière et modifier notre requête.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Calculez les n-grammes les plus courants \n",
    "\n",
    "    *Indice : la bibliothèque nltk peut être utile pour tokenizer (séparer en mots) le texte*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from nltk.util import everygrams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_tokens(tweet_df):\n",
    "    \"\"\"\n",
    "    Helper function to generate a list of text tokens from concatenating\n",
    "    all of the text contained in Tweets \n",
    "    \"\"\"\n",
    "    # concat entire corpus\n",
    "    all_text = ' '.join((txt for txt in tweet_df['text'] if type(txt) == str))\n",
    "    # tokenize\n",
    "    tokens = (TweetTokenizer(preserve_case=False,\n",
    "                            reduce_len=True,\n",
    "                            strip_handles=False)\n",
    "              .tokenize(all_text))\n",
    "    # remove symbol-only tokens for now\n",
    "    tokens = [tok for tok in tokens if not tok in string.punctuation]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = get_all_tokens(tweet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate a range of ngrams using some handy functions\n",
    "top_grams = Counter(everygrams(tokens, min_len=2, max_len=4))\n",
    "\n",
    "top_grams.most_common(25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Quelles informations peut-on retrouver sur les JO de Tokyo en explorant les données ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but, ici, est de passer de données semi-structurées (csv) à la matrice de données bidimensionnelle d'observations et de caractéristiques attendue par de nombreuses bibliothèques de ML."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L''objectif de ce TD est de rassembler utilisateurs de Twitter en groupes de similarité. De nombreuses informations peuvent être disponibles via l'API de Twitter, pour plus de simplicité nous nous concentrerons sur la description des utilisateurs. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Les liens URL ne sont pas porteurs de beaucoup d'informations (raccourcis par twitter). Commençons par enlever les URL du texte et les remplacer par un pattern unique, `\"lien_url\"`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une étape importante du traitement de texte consiste à diviser la string en tokens (ou mots). Il existe de nombreuses façons de découper une chaîne de texte en tokens, mais dans le cadre de cette discussion, nous nous intéresserons principalement à l'anglais. Dans ce cas, le découpage du texte via les espaces reste la manière la plus simple de procéder. \n",
    "\n",
    "Nous pouvons également choisir de créer notre propre tokenizer explicite si les données (et la tâche) l'exigent. Une méthode particulière qui fonctionne avec les données Twitter est le TweetTokenizer de NLTK. Il préserve les symboles @ et # au début des mots, et peut également \"réduire\" les caractères répétés - c'est-à-dire que lolll, lollllll, et lollllllllllll se réduiront tous à la même représentation \"lolll\" (trois \"l\"). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Créer une fonction `my_tokenizer()` qui sépare du texte en tokens."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Enlever les \"stop-words\" (mots les plus courants, qui ne sont pas vraiment porteurs d'information - 'the', etc.)\n",
    "\n",
    "    *Indice : utiliser des listes de stop-words déjà faites, comme celle de `python-stop-words`. Avez-vous pense à tous les langages pertinents ?*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorisation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La plupart des algorithmes de ML prêts à l'emploi, par exemple ceux de sklearn, attendent une entrée sous forme de matrice de données bidimensionnelle de valeurs numériques : observations (lignes) x caractéristiques (colonnes). Pour créer une représentation numérique de données textuelles, nous devons vectoriser les caractéristiques du texte (tokens)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Qu'est-ce que le TF-IDF d'un mot dans un corpus de texte ? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Créer un TF-IDF grâce à sklearn, en tirant parti des arguments `preprocessor`, `stop-words`, `tokenizer` et `max-features`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Commentez le résultat : pourquoi la matrice est si sparse (clairsemée) ? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Quelles autres features peut-on ajouter à nos données ? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sélection d'un modèle et de ses hyperparamètres"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De nombeaux modèles de clustering sont disponibles via la bibliothèque `scikit-learn`([https://scikit-learn.org/stable/modules/clustering.html])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KMeans est un choix courant car il est très rapide pour des quantités modérées de données. Comme la plupart des algorithmes de ML, KMeans a des hyperparamètres qui doivent être choisis de manière appropriée. Dans le cas présent, ce paramètre est k, le nombre de cluster dans nos données."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En non supervisé, il n'est pas facile de calculer (et d'optimiser) un score de précision. Nous devons donc utiliser d'autres techniques pour comparer les modèles entre eux afin de sélectionner k. Comme nous ne connaissons pas ce nombre a priori, une technique consiste à comparer la valeur d'une certaine métrique de qualité parmi une gamme de k potentiels. Il existe un certain nombre de mesures de qualité connues, dont nous n'utiliserons que deux : le silhouette score et l'inertie."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Que représente le silhouette score ? Et l'inertie ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. En utilisant `scikit-learn`, calculer le silhouette-score ainsi que l'inertie pour plusieurs valeurs de k différentes : [2, 50, 200, 500] (cette étape peut prendre quelques minutes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. En utilisant `matplotlib.pyplot`, afficher l'évolution du silhouette-score et de l'inertie en fonction de la valeur de k"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Analyser les résultats ci-dessus : quelle serait la meilleure valeur de k ? Comment pourrait-on affiner cette recherche ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Entraîner le modèle, avec les bonnes valeurs de k, sur l'ensemble du jeu de données"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résultats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Combien y a-t-il d'utilisateurs dans chaque cluster ? Que peut-on en déduire ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rappelons que les utilisateurs sont représentés dans un espace de paramètres comprenant les mots utilisés dans leur biographie. Dans l'algorithme KMeans, la représentation résultante de ces cluster est constituée des coordonnées du centroïde de chaque cluster dans cet espace de tokens. Ainsi, une autre façon d'examiner nos résultats est de poser la question suivante : pour chaque centroïde de cluster, quels sont les tokens qui ont la plus grande projection sur ce centroïde ? En d'autres termes, quels sont les tokens les plus fortement associés à chaque groupe ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Afficher les n features les plus importantes pour chaque cluster"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Pour un cluster donné, afficher quelques biographies et analyser les résultats. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. (optionnel) : utiliser `MulticoreTSNE`pour résuire la dimension de la représentation des utilisateurs, et représenter en deux dimensions les cluster avec les fonction suivante : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plottable_df(users, bios, two_d_coords, labels):\n",
    "    \"\"\"\n",
    "    Combine the necessary pieces of data to create a data structure that plays\n",
    "    nicely with the our 2d tsne chart.\n",
    "\n",
    "    Note: assumes that all argument data series\n",
    "    are in the same order e.g. the first user, bio, coords, and label\n",
    "    all correspond to the same user.\n",
    "    \"\"\"\n",
    "    # set up color palette\n",
    "    num_labels = len(set(labels))\n",
    "    colors = sns.color_palette('hls', num_labels).as_hex()\n",
    "    color_lookup = {v:k for k,v in zip(colors, set(labels))}\n",
    "    # combine data into a single df\n",
    "    df = pd.DataFrame({'uid': users,\n",
    "                       'text': bios,\n",
    "                       'label': labels,\n",
    "                       'x_val': two_d_coords[:,0],\n",
    "                       'y_val': two_d_coords[:,1],\n",
    "                      })\n",
    "    # convert labels to colors\n",
    "    df['color'] = list(map(lambda x: color_lookup[x], labels))\n",
    "    return df\n",
    "\n",
    "def plot_tsne(df, title='t-SNE plot'):\n",
    "    # add our DataFrame as a ColumnDataSource for Bokeh\n",
    "    plot_data = ColumnDataSource(df)\n",
    "    # configure the chart\n",
    "    tsne_plot = figure(title=title, plot_width=800, plot_height=700, tools=('pan, box_zoom, reset'))\n",
    "    # add a hover tool to display words on roll-over\n",
    "    tsne_plot.add_tools(\n",
    "        HoverTool(tooltips = \"\"\"<div style=\"width: 400px;\">(@label) @text</div>\"\"\")\n",
    "    )\n",
    "    # draw the words as circles on the plot\n",
    "    tsne_plot.circle('x_val', 'y_val',\n",
    "                     source=plot_data,\n",
    "                     color='color',\n",
    "                     line_alpha=0.2,\n",
    "                     fill_alpha=0.1,\n",
    "                     size=7,\n",
    "                     hover_line_color='black')\n",
    "    # configure visual elements of the plot\n",
    "    tsne_plot.title.text_font_size = '12pt'\n",
    "    tsne_plot.xaxis.visible = False\n",
    "    tsne_plot.yaxis.visible = False\n",
    "    tsne_plot.grid.grid_line_color = None\n",
    "    tsne_plot.outline_line_color = None\n",
    "    return tsne_plot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Itération"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Le choix d'un modèle dépend beaucoup des hypothèses faites sur la répartition des données initiales. Quel serait le modèle le plus pertinent selon vous ? Vous pouvez vous référer à [la page d'acceuil scikit learn](https://scikit-learn.org/stable/modules/clustering.html). Réitérer le processus ci dessus avec le modèle choisi."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Quelles améliorations pourrait-on faire sur le traitement des données ? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
